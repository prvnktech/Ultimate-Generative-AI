{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "944592fd-ef6c-4c39-986a-62ff2bc9ddc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>üìè Simple Information Theory Demo</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b05975d30384be197a9e8788221ed45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HTML(value='<b>Adjust Weights (Distribution is auto-normalized):</b>'), FloatSlid‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import entropy\n",
    "from scipy.special import rel_entr\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- Core Logic ---\n",
    "def calculate_entropy(probs):\n",
    "    \"\"\"Calculates Shannon Entropy in bits (base 2).\"\"\"\n",
    "    p = np.array(probs)\n",
    "    # Avoid division by zero if all weights are 0 (edge case)\n",
    "    if np.sum(p) == 0:\n",
    "        return 0.0, p\n",
    "    p = p / np.sum(p)\n",
    "    return entropy(p, base=2), p\n",
    "\n",
    "def calculate_kl(p_probs, q_probs):\n",
    "    \"\"\"Calculates KL Divergence.\"\"\"\n",
    "    return np.sum(rel_entr(p_probs, q_probs))\n",
    "\n",
    "# --- Interactive UI Class ---\n",
    "class InfoTheoryDemo:\n",
    "    def __init__(self):\n",
    "        self.out_entropy = widgets.Output()\n",
    "        self.out_kl = widgets.Output()\n",
    "\n",
    "        # --- Entropy Controls ---\n",
    "        # continuous_update=False ensures the graph only updates when you release the mouse\n",
    "        self.w1 = widgets.FloatSlider(value=2, min=0.1, max=10, description='Weight A:', continuous_update=False)\n",
    "        self.w2 = widgets.FloatSlider(value=5, min=0.1, max=10, description='Weight B:', continuous_update=False)\n",
    "        self.w3 = widgets.FloatSlider(value=3, min=0.1, max=10, description='Weight C:', continuous_update=False)\n",
    "        \n",
    "        # --- KL Divergence Controls ---\n",
    "        self.p_slider = widgets.FloatSlider(value=0.4, min=0.01, max=0.99, step=0.01, description='P(Event 1):', continuous_update=False)\n",
    "        self.q_slider = widgets.FloatSlider(value=0.5, min=0.01, max=0.99, step=0.01, description='Q(Event 1):', continuous_update=False)\n",
    "        \n",
    "        # --- Bind Events ---\n",
    "        self.w1.observe(self.update_entropy, names='value')\n",
    "        self.w2.observe(self.update_entropy, names='value')\n",
    "        self.w3.observe(self.update_entropy, names='value')\n",
    "        \n",
    "        self.p_slider.observe(self.update_kl, names='value')\n",
    "        self.q_slider.observe(self.update_kl, names='value')\n",
    "\n",
    "        # Run initial updates\n",
    "        self.update_entropy(None)\n",
    "        self.update_kl(None)\n",
    "\n",
    "    def update_entropy(self, change):\n",
    "        with self.out_entropy:\n",
    "            self.out_entropy.clear_output(wait=True)\n",
    "            \n",
    "            # 1. Calculate new values\n",
    "            raw = [self.w1.value, self.w2.value, self.w3.value]\n",
    "            ent_val, p_norm = calculate_entropy(raw)\n",
    "            \n",
    "            # 2. Re-draw Figure\n",
    "            fig = go.Figure(data=[go.Bar(\n",
    "                x=['A', 'B', 'C'],\n",
    "                y=p_norm,\n",
    "                text=np.round(p_norm, 2),\n",
    "                textposition='auto',\n",
    "                marker_color=['#FF9999', '#66B2FF', '#99FF99']\n",
    "            )])\n",
    "            fig.update_layout(\n",
    "                title=f\"Probability Distribution<br>Entropy: {ent_val:.4f} bits\",\n",
    "                yaxis_title=\"Probability\",\n",
    "                yaxis_range=[0, 1],\n",
    "                height=300,\n",
    "                margin=dict(l=20, r=20, t=60, b=20)\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "    def update_kl(self, change):\n",
    "        with self.out_kl:\n",
    "            self.out_kl.clear_output(wait=True)\n",
    "            \n",
    "            # 1. Calculate new values\n",
    "            p_val = self.p_slider.value\n",
    "            q_val = self.q_slider.value\n",
    "            \n",
    "            P = [p_val, 1 - p_val]\n",
    "            Q = [q_val, 1 - q_val]\n",
    "            \n",
    "            kl_val = calculate_kl(P, Q)\n",
    "            \n",
    "            # 2. Re-draw Figure\n",
    "            fig = go.Figure(data=[\n",
    "                go.Bar(name='P (Reference)', x=['Event 1', 'Event 2'], y=P, marker_color='#66B2FF'),\n",
    "                go.Bar(name='Q (Approximation)', x=['Event 1', 'Event 2'], y=Q, marker_color='#FF9999')\n",
    "            ])\n",
    "            fig.update_layout(\n",
    "                title=f\"Comparing Distributions P vs Q<br>KL Divergence: {kl_val:.4f} nats\",\n",
    "                barmode='group',\n",
    "                yaxis_range=[0, 1],\n",
    "                height=300,\n",
    "                margin=dict(l=20, r=20, t=60, b=20)\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "    def render(self):\n",
    "        # Layout for Entropy Tab\n",
    "        ent_ui = widgets.VBox([\n",
    "            widgets.HTML(\"<b>Adjust Weights (Distribution is auto-normalized):</b>\"),\n",
    "            self.w1, self.w2, self.w3, \n",
    "            self.out_entropy\n",
    "        ])\n",
    "        \n",
    "        # Layout for KL Tab\n",
    "        kl_ui = widgets.VBox([\n",
    "            widgets.HTML(\"<b>Adjust Probability of Event 1 for P and Q:</b>\"),\n",
    "            self.p_slider, self.q_slider, \n",
    "            self.out_kl\n",
    "        ])\n",
    "        \n",
    "        # Create Tabs\n",
    "        tabs = widgets.Tab(children=[ent_ui, kl_ui])\n",
    "        tabs.set_title(0, 'Entropy Calculator')\n",
    "        tabs.set_title(1, 'KL Divergence')\n",
    "        \n",
    "        display(HTML(\"<h3>üìè Simple Information Theory Demo</h3>\"))\n",
    "        display(tabs)\n",
    "\n",
    "# --- Run the App ---\n",
    "app = InfoTheoryDemo()\n",
    "app.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665dd01-4e41-4066-9fef-d7064b99afad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d8615-a282-40dc-b439-600aaad61ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
