{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d6d613-13cc-4f73-bb70-0c485d1b8f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ce9619433d486d9b0f665748aea077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>üìâ Simple GPT Fine-Tuning Lab</h3>'), HBox(children=(IntSlider(value=3, continuo‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# HELPER: Simulation Logic\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_simulated_loss(epochs, noise_level=0.1):\n",
    "    \"\"\"Generates a synthetic loss curve for demonstration.\"\"\"\n",
    "    steps = np.linspace(0, epochs, 30)\n",
    "    # Exponential decay to simulate learning\n",
    "    loss = 2.5 * np.exp(-0.5 * steps) \n",
    "    # Add random noise\n",
    "    loss += np.random.normal(0, noise_level, len(steps))\n",
    "    return np.clip(loss, 0.1, 5.0)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# APP CLASS: Minimal UI\n",
    "# -----------------------------------------------------------------------------\n",
    "class SimpleFineTuner:\n",
    "    def __init__(self):\n",
    "        self.output_area = widgets.Output()\n",
    "        \n",
    "        # --- Controls ---\n",
    "        self.epoch_slider = widgets.IntSlider(\n",
    "            value=3, min=1, max=10, \n",
    "            description='Epochs:',\n",
    "            continuous_update=False\n",
    "        )\n",
    "        \n",
    "        self.batch_select = widgets.Dropdown(\n",
    "            options=[4, 8, 16, 32], \n",
    "            value=8, \n",
    "            description='Batch Size:'\n",
    "        )\n",
    "        \n",
    "        # --- Buttons ---\n",
    "        self.btn_train = widgets.Button(\n",
    "            description=\"Run Simulation\", \n",
    "            button_style='success', # Green\n",
    "            icon='play'\n",
    "        )\n",
    "        self.btn_code = widgets.Button(\n",
    "            description=\"Show Requirement Code\", \n",
    "            button_style='info',    # Blue\n",
    "            icon='code'\n",
    "        )\n",
    "        \n",
    "        # --- Event Binding ---\n",
    "        self.btn_train.on_click(self.run_simulation)\n",
    "        self.btn_code.on_click(self.show_code)\n",
    "        \n",
    "    def show_code(self, b):\n",
    "        \"\"\"Displays the raw code required for the assignment.\"\"\"\n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            print(\"=== üìù REQUIREMENT: FINE-TUNING CODE PATTERN ===\")\n",
    "            print(\"This code demonstrates how to use Hugging Face for the task:\\n\")\n",
    "            \n",
    "            code_str = \"\"\"\n",
    "# 1. Dataset Preparation\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 2. Model & Tokenizer Setup\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token # GPT-2 needs a pad token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 3. Preprocessing\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 4. Training Configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8\n",
    ")\n",
    "\n",
    "# 5. Initialize & Run Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "            \"\"\"\n",
    "            print(code_str)\n",
    "\n",
    "    def run_simulation(self, b):\n",
    "        \"\"\"Runs the mock training loop.\"\"\"\n",
    "        epochs = self.epoch_slider.value\n",
    "        \n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            print(f\"üöÄ Starting Mock Training ({epochs} Epochs)...\")\n",
    "            \n",
    "            # Generate fake data\n",
    "            loss_curve = get_simulated_loss(epochs)\n",
    "            progress = widgets.FloatProgress(min=0, max=len(loss_curve), description=\"Training:\")\n",
    "            display(progress)\n",
    "            \n",
    "            # Simulation Loop\n",
    "            for i, loss in enumerate(loss_curve):\n",
    "                progress.value = i + 1\n",
    "                if i % 5 == 0:\n",
    "                    print(f\"Step {i*10}: Loss = {loss:.4f}\")\n",
    "                time.sleep(0.1) # Simulate computation time\n",
    "            \n",
    "            # Final Result\n",
    "            print(\"\\n‚úÖ Training Complete.\")\n",
    "            \n",
    "            # Plot\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(loss_curve, marker='o', linestyle='-', color='teal', label='Training Loss')\n",
    "            plt.title(\"Simulated Loss Curve\")\n",
    "            plt.xlabel(\"Steps\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Displays the UI elements.\"\"\"\n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h3>üìâ Simple GPT Fine-Tuning Lab</h3>\"),\n",
    "            widgets.HBox([self.epoch_slider, self.batch_select]),\n",
    "            widgets.HBox([self.btn_train, self.btn_code]),\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            self.output_area\n",
    "        ]))\n",
    "\n",
    "# --- Run Application ---\n",
    "if __name__ == \"__main__\":\n",
    "    app = SimpleFineTuner()\n",
    "    app.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416264d0-4fcd-40f0-b2b1-bc3fcb8f0e00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
